{
    "PROJECT_NAME": "Diabetes Multiclassification",
    "GITHUB_USERNAME": "Sambonic",
    "REPO_NAME": "diabetes-multiclassification",
    "PROJECT_DESCRIPTION": "Diabetes multiclassification using various machine learning algorithms ",
    "PROJECT_FEATURES": "* **Multiclass Diabetes Classification:** Predicts diabetes severity (no diabetes, prediabetes, diabetes) using machine learning.\n* **Data Preprocessing:** Handles missing values using mean/mode imputation and outlier detection/adjustment for BMI.  Categorical features are handled using mode imputation and SMOTENC for oversampling.\n* **Feature Engineering:** Discretizes BMI into meaningful categories.\n* **Feature Selection:** Employs Chi-squared test and Random Forest feature importance to select relevant features.\n* **Model Training:**  Trains and evaluates several classification models: Random Forest, Decision Tree, LightGBM, and Logistic Regression.  Learning curves are plotted for each model.\n* **Model Evaluation:** Assesses model performance using accuracy, precision, recall, F1-score, confusion matrices, and ROC curves. Hyperparameter tuning is performed using RandomizedSearchCV to optimize model performance.\n* **Class Imbalance Handling:** Addresses class imbalance through undersampling and SMOTENC oversampling techniques.\n* **Comparative Analysis:** Compares the performance of different models with and without feature selection and hyperparameter optimization.  Results are visualized through bar charts.\n\n",
    "PROJECT_USAGE": "## Diabetes Multi-Classification Project Walkthrough\n\nThis project performs multi-class classification on a diabetes dataset to predict the severity of diabetes (no diabetes, pre-diabetes, diabetes).  The steps below outline the process, including data preprocessing, feature selection, model training, and evaluation.\n\n**1. Data Loading and Exploration:**\n\nThe notebook begins by loading the diabetes dataset from a CSV file (`../datasets/diabetes_012_health_indicators_BRFSS2015.csv`). It then explores the data using descriptive statistics (`df.describe()`, `df.dtypes`, `df.nunique()`) and visualizes the distribution of features and their relationships with the target variable ('Diabetes_012') using histograms, box plots, count plots, and a correlation heatmap.  The code prints out the initial shapes, data types, and number of unique values for each feature.  It also identifies missing values and duplicates.\n\n**2. Data Preprocessing:**\n\n* **Handling Missing Values:** The notebook explores two imputation strategies:\n    * **Mean/Mode Imputation:** Missing values in numerical features are filled with the mean, and those in categorical features are filled with the mode.  This is performed separately for the subset of classes {1, 2} and {0}.\n    * **Dropping Missing Values:** Rows with missing values are dropped.\n\n   The method achieving the highest accuracy using RandomForestClassifier on a subset of the data is selected.\n\n* **Handling Duplicates:** Duplicate rows are removed from the dataset.\n\n* **Outlier Handling:** Outliers in the 'BMI' feature are detected using the IQR method and are capped at the upper and lower bounds.\n\n* **Discretization:** 'BMI' is discretized into meaningful categories (underweight, healthy weight, overweight, obesity classes).\n\n* **Class Imbalance Handling:**  The dataset suffers from class imbalance. This is addressed with a combination of undersampling the majority class (class 0) and oversampling the minority classes (classes 1 and 2) using SMOTENC (to handle both categorical and numerical features).\n\n**3. Feature Selection:**\n\nTwo feature selection methods are employed:\n\n* **Chi-Squared Test:**  This statistical test identifies features most associated with the target variable. The top `k` features are selected based on their Chi-squared scores.\n\n* **Random Forest Feature Importance:**  Feature importances are extracted from a trained Random Forest model. The top `k` features are selected based on these importances.\n\nFinally, the intersection of features selected by both methods is used as the final set of features.\n\n**4. Model Training and Evaluation:**\n\nSeveral classification models are trained and evaluated: Random Forest, Decision Tree, LightGBM, and Logistic Regression.  For each model, the notebook performs the following:\n\n* **Learning Curve Plot:** This visualizes the model's performance as a function of training data size to help determine if the model is overfitting or underfitting.\n\n* **Model Training:** The model is trained on the training set.\n\n* **Hyperparameter Tuning:** (Commented-out, likely due to time constraints) RandomizedSearchCV would have been used to find optimal hyperparameters for each model.  The code shows pre-defined parameters from the notebook author.\n\n* **Model Evaluation:** The model is evaluated on the test set using accuracy, precision, recall, and F1-score. A confusion matrix and classification report are generated.\n\n* **ROC Curve:**  ROC curves are plotted for each model, comparing its performance across different classes.\n\nThe results for all models, with and without feature selection and hyperparameter optimization are stored in a dictionary for further analysis.\n\n\n**5. Results Comparison and Visualization:**\n\nFinally, the notebook presents a comparison of the performance of different models using bar charts for each metric (accuracy, precision, recall, F1-score). The comparison is made for models with and without feature selection and hyperparameter tuning.  Separate comparisons are also shown for each model type (Random Forest, Decision Tree, LightGBM, Logistic Regression).\n\n\n**To run the project:**\n\n1.  **Ensure you have the necessary libraries installed:**  (Already done, per your assumption)\n2.  **Open the Jupyter Notebook (`diabetes_classification_ml.ipynb`)**.\n3.  **Run all cells sequentially**. The notebook will output various plots and metrics showing the results of the analysis.  Note that the hyperparameter tuning sections are commented out; uncomment these sections to perform a full hyperparameter search for each model.  This will significantly increase the runtime.\n\nThe project uses a series of visualizations and analyses to build an effective model for predicting diabetes severity.  The results show the impact of different data preprocessing and feature selection methods on model performance.  The different model types show different characteristics and may be useful in different contexts.  The comparative analysis allows for an informed decision on which model is best suited for the given task.\n"
}